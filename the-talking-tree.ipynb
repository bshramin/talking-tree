{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "This notebook implements The Talking Tree. It trains a boosted tree model using BigQuery ML on the census_adult_income dataset to predict if an individual earns over $50K/year. Using BigQuery’s ML.EXPLAIN_PREDICT, it extracts decision paths, then leverages ML.GENERATE_TEXT to produce human-readable explanations (e.g., “High income predicted due to advanced education and long work hours”). The code runs entirely within BigQuery’s ecosystem, showcasing its ML and generative AI capabilities for interpretable AI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Create a new dedicated GCP project with a name like `treetalk`.\n",
    "\n",
    "**Create a Service Account:**\n",
    "Go to `IAM & Admin > Service Accounts`.\n",
    "Click `+ Create Service Account` at the top. Name it `treetalk`.\n",
    "Grant access to roles `BigQuery Admin` and `Vertex AI Administrator`.\n",
    "\n",
    "**Generate the JSON Key:**\n",
    "On the Service Accounts page, find your new service account (`treetalk@...`).\n",
    "Click the account, then go to the Keys tab.\n",
    "Click `Add Key > Create new key`.\n",
    "Select JSON as the key type and click Create.\n",
    "A json file will download to your computer. This is your service account JSON key. Save it securely.\n",
    "\n",
    "\n",
    "**Upload the JSON Key as a Dataset:**\n",
    "In the notebook, locate the Data panel on the right. Click Upload to create a new dataset. Name the dataset something like “Service Account Key” (keep it private for security). Drag and drop your JSON key file (e.g., treetalk-abcdef123456.json) or click to browse and select it. Click create. The dataset will appear under Data > Your Datasets in the notebook sidebar (path: /kaggle/input/bigquery-key/ or similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T18:55:20.594511Z",
     "iopub.status.busy": "2025-08-24T18:55:20.594196Z",
     "iopub.status.idle": "2025-08-24T18:55:20.599274Z",
     "shell.execute_reply": "2025-08-24T18:55:20.598025Z",
     "shell.execute_reply.started": "2025-08-24T18:55:20.594487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configs\n",
    "sa_key_file_path = '/kaggle/input/sa-key/treetalk-470016-9e5b0e9489cd.json'\n",
    "gcp_region = 'US'\n",
    "llm_model = 'gemini-2.0-flash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-24T17:08:01.046512Z",
     "iopub.status.busy": "2025-08-24T17:08:01.045921Z",
     "iopub.status.idle": "2025-08-24T17:08:04.808350Z",
     "shell.execute_reply": "2025-08-24T17:08:04.807141Z",
     "shell.execute_reply.started": "2025-08-24T17:08:01.046488Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "!pip install google-cloud-bigquery-storage\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import resourcemanager_v3\n",
    "from google.api_core import exceptions\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Convert warnings to errors, we don't want to have any warnings\n",
    "warnings.simplefilter('error', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T17:06:05.508712Z",
     "iopub.status.busy": "2025-08-24T17:06:05.508087Z",
     "iopub.status.idle": "2025-08-24T17:06:05.582011Z",
     "shell.execute_reply": "2025-08-24T17:06:05.581131Z",
     "shell.execute_reply.started": "2025-08-24T17:06:05.508681Z"
    }
   },
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    sa_key_file_path,\n",
    "    scopes=['https://www.googleapis.com/auth/cloud-platform']\n",
    ")\n",
    "\n",
    "gcp_project=credentials.project_id\n",
    "bq_client = bigquery.Client(credentials=credentials, project=gcp_project)\n",
    "print(gcp_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Explore the Dataset\n",
    "\n",
    "Query `the census_adult_income` dataset to understand its structure and select a sample record for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T17:06:05.583358Z",
     "iopub.status.busy": "2025-08-24T17:06:05.583018Z",
     "iopub.status.idle": "2025-08-24T17:06:07.439886Z",
     "shell.execute_reply": "2025-08-24T17:06:07.439108Z",
     "shell.execute_reply.started": "2025-08-24T17:06:05.583336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Query to preview the dataset\n",
    "query_explore = f\"\"\"\n",
    "SELECT *\n",
    "FROM `bigquery-public-data.ml_datasets.census_adult_income`\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "df_explore = bq_client.query(query_explore).to_dataframe()\n",
    "print(df_explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T17:40:25.626834Z",
     "iopub.status.busy": "2025-08-24T17:40:25.626497Z",
     "iopub.status.idle": "2025-08-24T17:40:27.145207Z",
     "shell.execute_reply": "2025-08-24T17:40:27.144201Z",
     "shell.execute_reply.started": "2025-08-24T17:40:25.626810Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select a sample record (e.g., index = 123) for prediction and explanation\n",
    "sample_index = 23300\n",
    "query_sample = f\"\"\"\n",
    "SELECT *\n",
    "FROM `bigquery-public-data.ml_datasets.census_adult_income`\n",
    "ORDER BY (SELECT NULL)\n",
    "LIMIT 1 OFFSET {sample_index}\n",
    "\"\"\"\n",
    "df_sample = bq_client.query(query_sample).to_dataframe()\n",
    "print(\"\\nSample Record for Prediction:\")\n",
    "print(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train the Boosted Tree Model\n",
    "\n",
    "Train a boosted tree classifier using BigQuery ML to predict income (> $50K or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T17:06:09.448563Z",
     "iopub.status.busy": "2025-08-24T17:06:09.448319Z",
     "iopub.status.idle": "2025-08-24T17:06:09.729649Z",
     "shell.execute_reply": "2025-08-24T17:06:09.728746Z",
     "shell.execute_reply.started": "2025-08-24T17:06:09.448543Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_id = f\"{gcp_project}.treetalk\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = gcp_region\n",
    "bq_client.create_dataset(dataset, exists_ok=True)\n",
    "print(f\"Dataset {dataset_id} is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T17:08:08.806351Z",
     "iopub.status.busy": "2025-08-24T17:08:08.805404Z",
     "iopub.status.idle": "2025-08-24T17:26:26.350935Z",
     "shell.execute_reply": "2025-08-24T17:26:26.349904Z",
     "shell.execute_reply.started": "2025-08-24T17:08:08.806289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if model already exists\n",
    "model_id = f\"{gcp_project}.treetalk.income_predictor\"\n",
    "try:\n",
    "    bq_client.get_model(model_id)\n",
    "    print(f\"Model {model_id} already exists. Skipping training.\")\n",
    "except exceptions.NotFound:\n",
    "    # Model doesn't exist, so create and train it\n",
    "    print(\"Creating the model.\")\n",
    "    query_train = f\"\"\"\n",
    "    CREATE MODEL `{model_id}`\n",
    "    OPTIONS(\n",
    "      model_type='BOOSTED_TREE_CLASSIFIER',\n",
    "      input_label_cols=['income_bracket'],\n",
    "      max_iterations=50\n",
    "    ) AS\n",
    "    SELECT *\n",
    "    FROM `bigquery-public-data.ml_datasets.census_adult_income`\n",
    "    WHERE income_bracket IS NOT NULL\n",
    "    \"\"\"\n",
    "    job = bq_client.query(query_train)\n",
    "    job.result()  # Wait for the query to complete\n",
    "    print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Prediction and Explanation\n",
    "\n",
    "Use ML.EXPLAIN_PREDICT to predict income for the sample record and extract feature attributions for the decision path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T18:48:17.006062Z",
     "iopub.status.busy": "2025-08-24T18:48:17.005701Z",
     "iopub.status.idle": "2025-08-24T18:48:18.853701Z",
     "shell.execute_reply": "2025-08-24T18:48:18.852769Z",
     "shell.execute_reply.started": "2025-08-24T18:48:17.006018Z"
    }
   },
   "outputs": [],
   "source": [
    "query_explain = f\"\"\"\n",
    "SELECT *\n",
    "FROM ML.EXPLAIN_PREDICT(\n",
    "  MODEL `{gcp_project}.treetalk.income_predictor`,\n",
    "  (SELECT * FROM `bigquery-public-data.ml_datasets.census_adult_income` \n",
    "   ORDER BY (SELECT NULL)\n",
    "   LIMIT 1 OFFSET {sample_index}),\n",
    "  STRUCT(3 AS top_k_features)\n",
    ")\n",
    "\"\"\"\n",
    "print(\"Generating explanation data.\")\n",
    "df_explain = bq_client.query(query_explain).to_dataframe()\n",
    "print(\"Explanation:\")\n",
    "print(df_explain)\n",
    "explanation_str = df_explain.to_string(index=False)\n",
    "print(\"Explanation string:\")\n",
    "print(explanation_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Human-Readable Narrative\n",
    "\n",
    "Use BigQuery’s generative AI (ML.GENERATE_TEXT) to create a plain-English explanation of the decision path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T18:11:20.545228Z",
     "iopub.status.busy": "2025-08-24T18:11:20.544704Z",
     "iopub.status.idle": "2025-08-24T18:11:21.286585Z",
     "shell.execute_reply": "2025-08-24T18:11:21.285797Z",
     "shell.execute_reply.started": "2025-08-24T18:11:20.545198Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Create a Vertex AI connection for BigQuery ML\"\"\"\n",
    "\n",
    "# This requires the BigQuery Connection API\n",
    "from google.cloud import bigquery_connection_v1 as bq_connection\n",
    "\n",
    "client = bq_connection.ConnectionServiceClient(credentials=credentials)\n",
    "parent = f\"projects/{gcp_project}/locations/{gcp_region}\"\n",
    "\n",
    "connection = bq_connection.Connection()\n",
    "connection.cloud_resource = bq_connection.CloudResourceProperties()\n",
    "\n",
    "request = bq_connection.CreateConnectionRequest(\n",
    "    parent=parent,\n",
    "    connection_id=\"vertex-ai-connection\",\n",
    "    connection=connection,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    result = client.create_connection(request=request)\n",
    "    print(f\"Created connection: {result.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating connection: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T18:55:24.501187Z",
     "iopub.status.busy": "2025-08-24T18:55:24.500862Z",
     "iopub.status.idle": "2025-08-24T18:55:26.872232Z",
     "shell.execute_reply": "2025-08-24T18:55:26.871325Z",
     "shell.execute_reply.started": "2025-08-24T18:55:24.501162Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset_id = f\"{gcp_project}.llm\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = gcp_region\n",
    "bq_client.create_dataset(dataset, exists_ok=True)\n",
    "print(f\"Dataset {dataset_id} is ready.\")\n",
    "\n",
    "# Then create/deploy your Gemini model\n",
    "create_model_query = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{gcp_project}.llm.gemini`\n",
    "REMOTE WITH CONNECTION `{gcp_project}.{gcp_region}.vertex-ai-connection`\n",
    "OPTIONS (\n",
    "  ENDPOINT = '{llm_model}'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Creating model...\")\n",
    "bq_client.query(create_model_query).result()\n",
    "print(\"Model created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T19:05:06.273154Z",
     "iopub.status.busy": "2025-08-24T19:05:06.272791Z",
     "iopub.status.idle": "2025-08-24T19:05:08.965922Z",
     "shell.execute_reply": "2025-08-24T19:05:08.965106Z",
     "shell.execute_reply.started": "2025-08-24T19:05:06.273129Z"
    }
   },
   "outputs": [],
   "source": [
    "query_narrative = f\"\"\"\n",
    "SELECT ml_generate_text_result AS narrative\n",
    "FROM ML.GENERATE_TEXT(\n",
    "  MODEL `{gcp_project}.llm.gemini`,\n",
    "  (SELECT CONCAT('Explain in simple terms and in short, why the model made this prediction: ', @explanation) AS prompt),\n",
    "  STRUCT(0.7 AS temperature, 500 AS max_output_tokens)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    query_parameters=[\n",
    "        bigquery.ScalarQueryParameter(\"explanation\", \"STRING\", explanation_str)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Execute the query\n",
    "try:    \n",
    "    df_narrative = bq_client.query(query_narrative, job_config=job_config).to_dataframe()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error executing query: {str(e)}\")\n",
    "\n",
    "if not df_narrative.empty:\n",
    "    try:\n",
    "        # 1. Get the JSON string from the DataFrame\n",
    "        json_string = df_narrative['narrative'].iloc[0]\n",
    "        \n",
    "        # 2. Parse the string into a Python dictionary\n",
    "        response_data = json.loads(json_string)\n",
    "        \n",
    "        # 3. Extract the text from the correct path in the dictionary\n",
    "        narrative_text = response_data['candidates'][0]['content']['parts'][0]['text']\n",
    "        \n",
    "        # 4. Print the clean text\n",
    "        print(narrative_text.strip())\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "        print(f\"❌ Error parsing the model's JSON response: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No explanation generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8131164,
     "sourceId": 12855509,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
